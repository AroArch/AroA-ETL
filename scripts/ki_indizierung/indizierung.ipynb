{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5bbffc-cb98-4832-9285-7debc2f32155",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0c51c9-1f7e-43ea-8efe-0f446a1547cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import asyncio\n",
    "from tenacity import retry, stop_after_attempt\n",
    "import tenacity\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a3fe96-6f96-434e-ad86-fce41630cd9b",
   "metadata": {},
   "source": [
    "## Init Gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ab9ad8-698c-4ec0-8f25-6cca6f338f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"BUCKETNAME\"\n",
    "file_path = \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53a34702-b8c0-4273-8d36-d46523bb1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client(\"TODO\")\n",
    "bucket = client.bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a81d39f-e904-4ee9-a096-f0279652644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [blob.name for blob in bucket.list_blobs(prefix=file_path)]\n",
    "images = sorted(blob_name for blob_name in images if blob_name[-4:] == \".jpg\" and (\"_1.jpg\" in blob_name or \"_\" not in blob_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce46e7e8-0c0e-4f00-a2c3-89318855fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, GenerationConfig\n",
    "vertexai.init(project=\"TODO\", location=\"europe-west1\")\n",
    "model_name = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ab97f2-9fa6-46dc-b5c2-0c9c23bd5678",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78547c18-cf48-4d73-83de-f4c835066cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes\n",
    "with open(\"clustering/clustering.pkl\", \"rb\") as f:\n",
    "    clustering = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a0d6fcf-8daa-4e9b-a84f-6bcc93eec98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'135465157', '138196792', '4954189', '4954213', '4956212', '4957390', 'other'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = set(clustering.values())\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639a1c4-34e9-40e4-9773-09d5d720ff5f",
   "metadata": {},
   "source": [
    "# vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6052b114-7063-4083-9f62-eb5a62ef4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.types import Content, CreateCachedContentConfig, HttpOptions, Part, GenerateContentConfig, ThinkingConfig\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b5002cd-ac8e-4372-b3fa-975bb2a3ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(project=\"its-ml\", location=\"europe-west1\", vertexai=True, http_options=HttpOptions(api_version=\"v1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "418ee4eb-3fd3-4937-96b3-144f9459a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32431139-72cf-4fb7-a62f-eea2d816cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_images = [f[:-4] for f in os.listdir(\"few_shot_prompt_images\") if \"jpg\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "903dfee2-9f54-46d6-ba0d-c2cb17af7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_prompt = \"\"\"\n",
    "Your job is to transcribe information of persons from an image document. \n",
    "First detect if the image contains a table like structure with multiple persons in it. If it does not, return \"FALSE\". \n",
    "You are very accurate and you do not halucinate. The output should be provided in csv format separated by ';'. <header>first name; last name; date of birth</header>. \n",
    "Provide the date of birth in dd.mm.yyyy format. Dates are in the range from 1846 to 1945. Leave the date empty if it is not present. Fill missing day in dd.mm.yyyy with 00. \n",
    "Fill missing Month in dd.mm.yyyy with 00. Ignore degrees like dr.,  med., ing. Return only the csv, or False based on the stated condition.\n",
    "\"\"\".replace(\"\\n\",\"\")\n",
    "\n",
    "def known_image_prompt(image_name):\n",
    "    with open(f\"few_shot_prompt_images/{image_name}.txt\",\"r\") as f:\n",
    "        output = \"\\n\".join([line for line in f])\n",
    "    return [\n",
    "        Part.from_text(text=\"<EXAMPLE>\\nINPUT:\\n\"),\n",
    "        Part.from_uri(f\"gs://{bucket_name}/prompt/{image_name}.jpg\", mime_type=\"image/jpeg\"),\n",
    "        Part.from_text(text=f\"OUTPUT:\\n{output}\"),\n",
    "        Part.from_text(text=\"</EXAMPLE>\")\n",
    "    ]\n",
    "\n",
    "few_shots = {max(re.findall(r\"\\d+\",image_name),key=len): known_image_prompt(image_name) for image_name in few_shot_images}\n",
    "\n",
    "prompt_single_column = \"\"\"\n",
    "The following document contains information of several persons. First detect if the image contains a table like structure with multiple persons in it. If it does not, return \"FALSE\". \n",
    "Otherwise, fully transcribe the first name, last name and date of birth of every person in each row of each major column. Transcribe each major column from top to bottom before transcribing the next major column.\n",
    "Output the transcription in csv format: The last names may be in alphabetic order. Use latin characters only for first names and last names. \n",
    "first name; last name; date of birth\n",
    "Provide the date of birth in dd.mm.yyyy format. Dates are in the range from 1846 to 1945. Leave the date empty if it is not present. Fill missing day in dd.mm.yyyy with 00. Fill missing Month in dd.mm.yyyy with 00. Ignore degrees like dr.,  med., ing. Return only the csv, or False based on the stated condition.\n",
    "\n",
    "INPUT:\n",
    "\"\"\".replace(\"\\n\",\"\")\n",
    "\n",
    "prompt_multi_column = \"\"\"\n",
    "The following document contains information of several persons. First detect if the image contains a table like structure with multiple persons in it. If it does not, return \"FALSE\". \n",
    "Otherwise, first subdivide the image into major columns in a way that each row in a major column contains information of only one person. \n",
    "Then fully transcribe the first name, last name and date of birth of every person in each row of each major column. Transcribe each major column from top to bottom before transcribing the next major column.\n",
    "Output the transcription in csv format: The last names may be in alphabetic order. Use latin characters only for first names and last names. \n",
    "first name; last name; date of birth\n",
    "Provide the date of birth in dd.mm.yyyy format. Dates are in the range from 1846 to 1945. Leave the date empty if it is not present. Fill missing day in dd.mm.yyyy with 00. Fill missing Month in dd.mm.yyyy with 00. Ignore degrees like dr.,  med., ing. Return only the csv, or False based on the stated condition.\n",
    "\n",
    "INPUT:\n",
    "\"\"\".replace(\"\\n\",\"\")\n",
    "\n",
    "prompt_for_class = {\n",
    "    '135465157': prompt_single_column, \n",
    "    '138196792': prompt_multi_column, \n",
    "    '4954189': prompt_single_column, \n",
    "    '4956212': prompt_multi_column, \n",
    "    '4957390': prompt_single_column, \n",
    "    'other': prompt_single_column,\n",
    "    '4954213': prompt_single_column,\n",
    "}\n",
    "\n",
    "\n",
    "def build_prompt(blob_name):\n",
    "    img_class = clustering[blob_name.split(\"/\")[-1]]\n",
    "    pre_prompt_parts = [\n",
    "        Part.from_text(text=pre_prompt),\n",
    "    ] \n",
    "    one_shot_parts = few_shots[img_class] if img_class != \"other\" else [] \n",
    "    prompt_parts = [\n",
    "        Part.from_text(text=prompt_for_class[img_class]),\n",
    "        Part.from_uri(\n",
    "            uri=f\"gs://{bucket_name}/{blob_name}\",\n",
    "            mime_type = \"image/jpeg\",\n",
    "        ),\n",
    "        Part.from_text(text=\"OUTPUT:\\n\")\n",
    "    ]\n",
    "    return pre_prompt_parts + one_shot_parts + prompt_parts \n",
    "\n",
    "\n",
    "\n",
    "generation_configs = [\n",
    "    GenerationConfig(temperature=0, max_output_tokens=15000),\n",
    "    GenerationConfig(temperature=0.2, max_output_tokens=15000),\n",
    "    GenerationConfig(temperature=0.4, max_output_tokens=15000),\n",
    "]\n",
    "\n",
    "default_generation_config = GenerationConfig(temperature=0, max_output_tokens=15000)\n",
    "\n",
    "default_big_generation_config = GenerationConfig(temperature=0, max_output_tokens=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67555f7-cf61-456d-8cf4-04065dfa438f",
   "metadata": {},
   "source": [
    "# Parse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a496cb13-e6d8-4b6a-82b2-9859e1dca4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aroa_etl.attribute_processing.string_utils import fix_name_uppercasing, fix_visual_character_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c427e7-bfe1-4948-b01a-de72a128388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaut_header = \"first name; last name; date of birth\"\n",
    "def parse_and_validate_response(response):\n",
    "    \"tries parse a response. If it cannot be parsed with this function, either retry the generation or try to repair the response.\"\n",
    "    response_text = response.candidates[0].content.parts[0].text\n",
    "    response_text = response_text[:-1] if response_text[-1] == \"\\n\" else response_text\n",
    "        \n",
    "    assert re.match(r\"^[a-zA-Z0-9\\n\\s\\;\\,\\.]+$\",response_text)\n",
    "    response_text = response_text.split(\"\\n\")\n",
    "    response_data = {\n",
    "        \"response\": response\n",
    "    }\n",
    "    if response_text[0].lower() != \"false\":\n",
    "        if \"first\" not in response_text[0]:\n",
    "            response_text = [defaut_header, *response_text]\n",
    "        response_df = pd.DataFrame([line.split(\";\") for line in response_text[1:]],columns=response_text[0].split(\";\"))\n",
    "        assert response_df.shape[1] == 3, f\"response data is malformed shape: {response_df.shape}\"\n",
    "        response_data[\"data\"] = response_df\n",
    "        response_data[\"is_table\"] = True\n",
    "    else:\n",
    "        response_data[\"is_table\"] = False   \n",
    "    return response_data\n",
    "\n",
    "def fix_line_field_num(cells):\n",
    "    cells = [cell.strip() for cell in cells]\n",
    "    \n",
    "    if len(cells) > 3:\n",
    "        cells = [cell for cell in cells if cell != \"\"]\n",
    "    if len(cells) > 3:\n",
    "        cells = [f\"{cells[0]} {cells[1]}\".strip(), *cells[2:]]\n",
    "    if len(cells) > 3:\n",
    "        cells = [f\"{cells[0]} {cells[1]}\".strip(), *cells[2:]]\n",
    "    if len(cells) < 3:\n",
    "        cells = [*cells, *[\"\" for i in range(0,3-len(cells))]]\n",
    "    date_idx = [idx for idx, cell in enumerate(cells) if len(cell)>=1 and len(re.findall(r\"\\d\",cell))/len(cell)>0.3]\n",
    "    if len(date_idx)>0 and date_idx[0] ==1:\n",
    "        cells = [\"\"] + cells[:2]\n",
    "    elif len(date_idx)>0 and date_idx[0] ==0:\n",
    "        cells = [\"\", \"\"] + cells[:1]\n",
    "    return cells\n",
    "\n",
    "def convert_date_field(date):\n",
    "    date = \"\" if pd.isna(date) else date\n",
    "    date_components = date.split(\".\")\n",
    "    if len(date_components) != 3 or len(re.findall(r\"[^\\d\\.]\",date))>0:\n",
    "        return \"00000000\"\n",
    "    dd,mm,yyyy = date_components\n",
    "    if len(yyyy) == 2:\n",
    "        if int(yyyy) <= 45:\n",
    "            yyyy = f\"19{yyyy}\"\n",
    "        else:\n",
    "            yyyy = f\"18{yyyy}\"\n",
    "    elif len(yyyy) != 4 or int(yyyy) < 1845 or int(yyyy) > 1945:\n",
    "        yyyy = \"0000\"\n",
    "    if len(dd) == 1:\n",
    "        dd = f\"0{dd}\"\n",
    "    if len(mm) == 1:\n",
    "        mm = f\"0{mm}\"\n",
    "    return f\"{yyyy}{mm}{dd}\"\n",
    "    \n",
    "def remove_wrong_inserted_whitespace(name):\n",
    "    tokens = list(re.finditer(r\"[a-zA-Zäöüß]+\\s\", name))\n",
    "    remove_at = [w1.span()[1] \n",
    "                 for w1,w2 in zip(tokens, tokens[1:]) \n",
    "                 if len(w1.group())<= 3 and len(w2.group())<=3]\n",
    "    name = \"\".join([c for idx, c in enumerate(name) if idx not in remove_at])\n",
    "    return name\n",
    "\n",
    "def postprocess_year(y):\n",
    "    if len(y) == 3:\n",
    "        y = int(y)*10\n",
    "        if 1855< y and y< 1954:\n",
    "            return str(y)\n",
    "        else:\n",
    "            return \"0000\"\n",
    "    elif len(y) == 4:\n",
    "        y = int(y)\n",
    "        if 1855< y and y< 1954:\n",
    "            return str(y)\n",
    "        else:\n",
    "            if 1855< y + 800 and y+800< 1954:\n",
    "                return str(y+800)\n",
    "            elif 1855< y + 900 and y+900< 1954:\n",
    "                return str(y+900)\n",
    "        return \"0000\"\n",
    "    return \"0000\"\n",
    "def postprocess_day(d):\n",
    "    if pd.notna(d) and len(d) in [1,2] and int(d) <=31:\n",
    "        return d\n",
    "    else:\n",
    "        return \"00\"\n",
    "def postprocess_month(m):\n",
    "    if pd.notna(m) and len(m) in [1,2] and int(m) <=12:\n",
    "        return m\n",
    "    else:\n",
    "        return \"00\"\n",
    "        \n",
    "def repair_date(date):\n",
    "    date = str(date)\n",
    "    if re.search(r\"[a-zA-Z]\",date):\n",
    "        return \"00000000\"\n",
    "    dates = date.split(\".\")\n",
    "    if len(dates) != 3:\n",
    "        return \"0000000\"\n",
    "    dates[0] = postprocess_day(dates[0])\n",
    "    dates[1] = postprocess_month(dates[1])\n",
    "    dates[2] = postprocess_year(dates[2])\n",
    "    return \".\".join(dates)\n",
    "\n",
    "def remove_numbers_from_string(string):\n",
    "    return re.sub(r\"\\d\",\"\",string)\n",
    "    \n",
    "def parse_repair_and_validate_response(response):\n",
    "    \"tries to repair a response that cannot be parsed directly\"\n",
    "    if not isinstance(response, str):\n",
    "        response_text = response.candidates[0].content.parts[0].text\n",
    "    else: \n",
    "        response_text = response\n",
    "    response_text = response_text[:-1] if response_text[-1] == \"\\n\" else response_text\n",
    "    response_text = response_text.split(\"\\n\")\n",
    "    if \"first\" not in response_text[0]:\n",
    "        response_text = [defaut_header, *response_text]\n",
    "    if \"first\" in response_text[1]:\n",
    "        response_text = response_text[1:]\n",
    "        \n",
    "    columns=response_text[0].split(\";\")\n",
    "\n",
    "    data = [line.split(\";\") for line in response_text[1:]]\n",
    "    # fix to many fields in a row\n",
    "    data = [ fix_line_field_num(line) for line in data]\n",
    "   \n",
    "    data = pd.DataFrame(data,columns=response_text[0].split(\";\"))\n",
    "\n",
    "    data.iloc[:,0] = data.iloc[:,0].apply(lambda name: fix_name_uppercasing(fix_visual_character_decoding(name)))\n",
    "    data.iloc[:,0] = data.iloc[:,0].apply(lambda name: fix_name_uppercasing(remove_numbers_from_string(name)))\n",
    "    data.iloc[:,1] = data.iloc[:,1].apply(lambda name: fix_name_uppercasing(fix_visual_character_decoding(name)))\n",
    "    data.iloc[:,1] = data.iloc[:,1].apply(lambda name: fix_name_uppercasing(remove_numbers_from_string(name)))\n",
    "    \n",
    "    # remove noice \n",
    "    data.iloc[:,0] = data.iloc[:,0].apply(lambda name: \"\" if len(re.findall(\"[a-zA-Z]\",name))==0 else name)\n",
    "    data.iloc[:,1] = data.iloc[:,1].apply(lambda name: \"\" if len(re.findall(\"[a-zA-Z]\",name))==0 else name)\n",
    "    \n",
    "    # multiple 2 sillable names\n",
    "    data.iloc[:,0] = data.iloc[:,0].apply(lambda name: fix_name_uppercasing(remove_wrong_inserted_whitespace(name))) \n",
    "    # multiple first names and no last name\n",
    "    first_names = data.iloc[:,0].apply(lambda name: re.findall(r\"[a-zA-Zäöß]+\",name))\n",
    "    cond = (first_names.apply(len) >= 2) & (data.iloc[:,1].apply(len) == 0)\n",
    "    data.iloc[cond,0] = first_names.loc[cond].apply(lambda names: \" \".join(names[:-1]))\n",
    "    data.iloc[cond,1] = first_names.loc[cond].apply(lambda names: names[-1])\n",
    "\n",
    "    # restore alphabetic order if it is an ordered document\n",
    "    ## checks if a lastname is lectically smaller than the following last name\n",
    "    is_alphabetic = pd.Series([n1 <= n2 for n1,n2 in zip(list(data.iloc[:,1].values),list(data.iloc[1:,1].values))] + [True])\n",
    "    ## is an alphabetic ordered document\n",
    "    if is_alphabetic.sum() > 0.75 * is_alphabetic.shape[0] and (data.iloc[:,1] != \"\").sum() == data.shape[0]:\n",
    "        # correct first character of last names that contradict athe alphabetic order\n",
    "        next_first_char = list(data.iloc[:,1].apply(lambda name: name[0]).values)[1:]+[data.iat[-1,1][0]]\n",
    "        data.iloc[:,1] = [\n",
    "            name if is_alphabetic[idx] else\n",
    "            f\"{next_first_char[idx]}{name[1:]}\"\n",
    "            for idx, name in enumerate(data.iloc[:,1])\n",
    "        ] \n",
    "\n",
    "    # some documents have a date that is mistaken to be the birth date\n",
    "    most_frequent_date, occurrences = list(data.iloc[:,2].value_counts().items())[0]\n",
    "    if occurrences > 5:\n",
    "        data.iloc[:,2].replace(most_frequent_date, \"00.00.0000\")\n",
    "    # convert date from dd.mm.yyyy to yyyymmdd\n",
    "    data.iloc[:,2] = data.iloc[:,2].apply(lambda date: convert_date_field(date))\n",
    "    data.iloc[:,2] = data.iloc[:,2].apply(lambda date: repair_date(date))\n",
    "\n",
    "    # remove empty rows\n",
    "    empty = (data.iloc[:,0] == \"\") & (data.iloc[:,1] == \"\") & (data.iloc[:,2] == \"00000000\")\n",
    "    data = data.loc[~empty,:]\n",
    "    return data  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c44de-6b15-43cd-920f-12c64a01dee8",
   "metadata": {},
   "source": [
    "# async prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b94625c8-c94e-4995-9c2f-a3b16f34ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(len(generation_configs)+1))\n",
    "async def async_generate(job, generation_configs):\n",
    "    generation_config = None\n",
    "    parse = True\n",
    "    try:\n",
    "        generation_config = next(generation_configs)\n",
    "    except StopIteration:\n",
    "        generation_config = default_generation_config\n",
    "        parse = False\n",
    "    response = await model.generate_content_async(\n",
    "        job[\"content\"],\n",
    "        generation_config=generation_config,\n",
    "        #safety_settings=safety_settings,\n",
    "        stream=False,\n",
    "    )\n",
    "    job[\"response\"] = response\n",
    "    # do not parse after last retries\n",
    "    if not parse:\n",
    "        job[\"parsing_failure\"] = True\n",
    "        job[\"generate_failure\"] = False\n",
    "        return job\n",
    "    response_data = parse_and_validate_response(response)\n",
    "    job = {**job, **response_data}\n",
    "    job[\"parsing_failure\"] = False\n",
    "    job[\"generate_failure\"] = False\n",
    "    return job\n",
    "\n",
    "async def async_generate_attempt(job, generation_config):\n",
    "    response = await model.generate_content_async(\n",
    "        job[\"content\"],\n",
    "        generation_config=generation_config,\n",
    "        #safety_settings=safety_settings,\n",
    "        stream=False,\n",
    "    )\n",
    "    job[\"response\"] = response\n",
    "    return job\n",
    "\n",
    "def postprocess_response(job):\n",
    "    try:\n",
    "        response_data = parse_repair_and_validate_response(response)\n",
    "        job = {**job, **response_data}\n",
    "        job[\"parsing_failure\"] = False\n",
    "        job[\"generate_failure\"] = False\n",
    "    except:\n",
    "        job[\"parsing_failure\"] = True\n",
    "    return job\n",
    "    \n",
    "# careful. spawns all tasks up front. Create batches of function calls\n",
    "async def async_generate_batch(jobs):\n",
    "    # Create individual tasks for each prompt\n",
    "    get_responses = [\n",
    "        async_generate(j, iter(generation_configs)) \n",
    "        for j in jobs\n",
    "    ]\n",
    "    # Run all tasks concurrently\n",
    "    responses = await asyncio.gather(*get_responses,return_exceptions=True)\n",
    "    responses = [responses[i] if not isinstance(responses[i],tenacity.RetryError) else {**jobs[i], \"generate_failure\": True}\n",
    "                     for i in range(0,len(jobs))]\n",
    "    return responses\n",
    "\n",
    "async def async_generate_batch_no_rerun(jobs):\n",
    "    # Create individual tasks for each prompt\n",
    "    get_responses = [\n",
    "        async_generate_attempt(job, default_big_generation_config)\n",
    "        for j in jobs\n",
    "    ]\n",
    "    # Run all tasks concurrently\n",
    "    responses = await asyncio.gather(*get_responses,return_exceptions=True)\n",
    "    responses = [responses[i] if not isinstance(responses[i],tenacity.RetryError) else {**jobs[i], \"generate_failure\": True}\n",
    "                     for i in range(0,len(jobs))]\n",
    "    return responses\n",
    "#responses = await async_generate_batch([{\"image\":images[10],\"content\":build_prompt(images[10])}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5b016-8c4a-4dbe-97c1-cad84871ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost estimation\n",
    "# 15k images\n",
    "# input tokens per image 1200 Eingabe * 0.3 * 1/1M = 5.40\n",
    "# 1200 Ausgabe * 2.5 * 1/M = 45 Euro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7e85646-7df5-4046-9643-66676e8e1fdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start transcription of 17095 in 171 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                    | 0/17095 [20:49<?, ?it/s]\n",
      "\n",
      "\u001b[A%|▌                                                                                                       | 100/17095 [03:07<8:51:06,  1.88s/it]\n",
      "\u001b[A%|█▏                                                                                                      | 200/17095 [06:08<8:36:41,  1.83s/it]\n",
      "\u001b[A%|█▊                                                                                                      | 300/17095 [09:11<8:33:59,  1.84s/it]\n",
      "\u001b[A%|██▍                                                                                                     | 400/17095 [12:48<9:07:25,  1.97s/it]\n",
      "\u001b[A%|███                                                                                                    | 500/17095 [18:06<11:05:11,  2.41s/it]\n",
      "\u001b[A%|███▌                                                                                                   | 600/17095 [21:24<10:21:16,  2.26s/it]\n",
      "\u001b[A%|████▎                                                                                                   | 700/17095 [24:29<9:40:47,  2.13s/it]\n",
      "\u001b[A%|████▊                                                                                                   | 800/17095 [27:43<9:20:59,  2.07s/it]\n",
      "\u001b[A%|█████▍                                                                                                  | 900/17095 [31:37<9:40:27,  2.15s/it]\n",
      "\u001b[A%|██████                                                                                                 | 1000/17095 [35:04<9:30:13,  2.13s/it]\n",
      "\u001b[A%|██████▋                                                                                                | 1100/17095 [38:28<9:20:08,  2.10s/it]\n",
      "\u001b[A%|███████▏                                                                                               | 1200/17095 [41:24<8:48:40,  2.00s/it]\n",
      "\u001b[A%|███████▊                                                                                              | 1300/17095 [50:21<13:14:49,  3.02s/it]\n",
      "\u001b[A%|████████▎                                                                                             | 1400/17095 [53:02<11:18:09,  2.59s/it]\n",
      "\u001b[A%|█████████                                                                                              | 1500/17095 [54:36<9:04:27,  2.09s/it]\n",
      "\u001b[A%|█████████▋                                                                                             | 1600/17095 [56:41<7:55:19,  1.84s/it]\n",
      "\u001b[A%|██████████▏                                                                                            | 1700/17095 [59:50<7:56:31,  1.86s/it]\n",
      "\u001b[A%|██████████▋                                                                                          | 1800/17095 [1:02:54<7:52:10,  1.85s/it]\n",
      "\u001b[A%|███████████▏                                                                                         | 1900/17095 [1:06:13<7:59:29,  1.89s/it]\n",
      "\u001b[A%|███████████▊                                                                                         | 2000/17095 [1:08:53<7:34:01,  1.80s/it]\n",
      "\u001b[A%|████████████▍                                                                                        | 2100/17095 [1:11:48<7:26:34,  1.79s/it]\n",
      "\u001b[A%|████████████▉                                                                                        | 2200/17095 [1:14:25<7:07:57,  1.72s/it]\n",
      "\u001b[A%|█████████████▌                                                                                       | 2300/17095 [1:17:21<7:07:15,  1.73s/it]\n",
      "\u001b[A%|██████████████▏                                                                                      | 2400/17095 [1:19:39<6:38:26,  1.63s/it]\n",
      "\u001b[A%|██████████████▊                                                                                      | 2500/17095 [1:22:06<6:24:15,  1.58s/it]\n",
      "\u001b[A%|███████████████▎                                                                                     | 2600/17095 [1:24:50<6:26:26,  1.60s/it]\n",
      "\u001b[A%|███████████████▉                                                                                     | 2700/17095 [1:27:17<6:13:54,  1.56s/it]\n",
      "\u001b[A%|████████████████▌                                                                                    | 2800/17095 [1:29:50<6:09:48,  1.55s/it]\n",
      "\u001b[A%|█████████████████▏                                                                                   | 2900/17095 [1:32:31<6:11:02,  1.57s/it]\n",
      "\u001b[A%|█████████████████▋                                                                                   | 3000/17095 [1:34:34<5:44:38,  1.47s/it]\n",
      "\u001b[A%|██████████████████▎                                                                                  | 3100/17095 [1:36:37<5:25:36,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped 100 3100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A%|██████████████████▉                                                                                  | 3200/17095 [1:39:17<5:37:37,  1.46s/it]\n",
      "\u001b[A%|███████████████████▍                                                                                 | 3300/17095 [1:42:22<6:01:46,  1.57s/it]\n",
      "\u001b[A%|████████████████████                                                                                 | 3400/17095 [1:45:24<6:16:12,  1.65s/it]\n",
      "\u001b[A%|████████████████████▋                                                                                | 3500/17095 [1:48:25<6:24:29,  1.70s/it]\n",
      "\u001b[A%|█████████████████████▎                                                                               | 3600/17095 [1:50:29<5:51:13,  1.56s/it]\n",
      "\u001b[A%|█████████████████████▊                                                                               | 3700/17095 [1:53:21<5:58:42,  1.61s/it]\n",
      "\u001b[A%|██████████████████████▍                                                                              | 3800/17095 [1:56:14<6:04:32,  1.65s/it]\n",
      "\u001b[A%|███████████████████████                                                                              | 3900/17095 [1:59:13<6:10:56,  1.69s/it]\n",
      "\u001b[A%|███████████████████████▋                                                                             | 4000/17095 [2:01:57<6:05:18,  1.67s/it]\n",
      "\u001b[A%|████████████████████████▏                                                                            | 4100/17095 [2:04:56<6:10:20,  1.71s/it]\n",
      "\u001b[A%|████████████████████████▊                                                                            | 4200/17095 [2:08:04<6:18:12,  1.76s/it]\n",
      "\u001b[A%|█████████████████████████▍                                                                           | 4300/17095 [2:10:21<5:50:20,  1.64s/it]\n",
      "\u001b[A%|█████████████████████████▉                                                                           | 4400/17095 [2:14:17<6:33:11,  1.86s/it]\n",
      "\u001b[A%|██████████████████████████▌                                                                          | 4500/17095 [2:16:57<6:13:45,  1.78s/it]\n",
      "\u001b[A%|███████████████████████████▏                                                                         | 4600/17095 [2:19:47<6:05:49,  1.76s/it]\n",
      "\u001b[A%|███████████████████████████▊                                                                         | 4700/17095 [2:22:35<5:57:54,  1.73s/it]\n",
      "\u001b[A%|████████████████████████████▎                                                                        | 4800/17095 [2:24:58<5:36:24,  1.64s/it]\n",
      "\u001b[A%|████████████████████████████▉                                                                        | 4900/17095 [2:27:52<5:39:36,  1.67s/it]\n",
      "\u001b[A%|█████████████████████████████▌                                                                       | 5000/17095 [2:30:38<5:36:16,  1.67s/it]\n",
      "\u001b[A%|██████████████████████████████▏                                                                      | 5100/17095 [2:33:37<5:40:41,  1.70s/it]\n",
      "\u001b[A%|██████████████████████████████▋                                                                      | 5200/17095 [2:38:10<6:39:16,  2.01s/it]\n",
      "\u001b[A%|███████████████████████████████▎                                                                     | 5300/17095 [2:40:59<6:16:27,  1.92s/it]\n",
      "\u001b[A%|███████████████████████████████▉                                                                     | 5400/17095 [2:43:59<6:06:44,  1.88s/it]\n",
      "\u001b[A%|████████████████████████████████▍                                                                    | 5500/17095 [2:47:01<6:00:07,  1.86s/it]\n",
      "\u001b[A%|█████████████████████████████████                                                                    | 5600/17095 [2:49:54<5:49:19,  1.82s/it]\n",
      "\u001b[A%|█████████████████████████████████▋                                                                   | 5700/17095 [2:52:52<5:43:43,  1.81s/it]\n",
      "\u001b[A%|██████████████████████████████████▎                                                                  | 5800/17095 [2:55:36<5:31:06,  1.76s/it]\n",
      "\u001b[A%|██████████████████████████████████▊                                                                  | 5900/17095 [2:58:46<5:35:58,  1.80s/it]\n",
      "\u001b[A%|███████████████████████████████████▍                                                                 | 6000/17095 [3:01:31<5:24:42,  1.76s/it]\n",
      "\u001b[A%|████████████████████████████████████                                                                 | 6100/17095 [3:04:41<5:29:46,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped 3100 6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A%|████████████████████████████████████▋                                                                | 6200/17095 [3:07:17<5:13:34,  1.73s/it]\n",
      "\u001b[A%|█████████████████████████████████████▏                                                               | 6300/17095 [3:11:02<5:38:56,  1.88s/it]\n",
      "\u001b[A%|█████████████████████████████████████▊                                                               | 6400/17095 [3:13:47<5:23:34,  1.82s/it]\n",
      "\u001b[A%|██████████████████████████████████████▍                                                              | 6500/17095 [3:16:35<5:13:03,  1.77s/it]\n",
      "\u001b[A%|██████████████████████████████████████▉                                                              | 6600/17095 [3:19:27<5:07:20,  1.76s/it]\n",
      "\u001b[A%|███████████████████████████████████████▌                                                             | 6700/17095 [3:23:12<5:30:00,  1.90s/it]\n",
      "\u001b[A%|████████████████████████████████████████▏                                                            | 6800/17095 [3:25:46<5:07:59,  1.80s/it]\n",
      "\u001b[A%|████████████████████████████████████████▊                                                            | 6900/17095 [3:27:41<4:32:22,  1.60s/it]\n",
      "\u001b[A%|█████████████████████████████████████████▎                                                           | 7000/17095 [3:30:01<4:19:32,  1.54s/it]\n",
      "\u001b[A%|█████████████████████████████████████████▉                                                           | 7100/17095 [3:33:20<4:39:08,  1.68s/it]\n",
      "\u001b[A%|██████████████████████████████████████████▌                                                          | 7200/17095 [3:36:13<4:38:55,  1.69s/it]\n",
      "\u001b[A%|███████████████████████████████████████████▏                                                         | 7300/17095 [3:38:59<4:34:40,  1.68s/it]\n",
      "\u001b[A%|███████████████████████████████████████████▋                                                         | 7400/17095 [3:41:38<4:27:16,  1.65s/it]\n",
      "\u001b[A%|████████████████████████████████████████████▎                                                        | 7500/17095 [3:44:33<4:29:25,  1.68s/it]\n",
      "\u001b[A%|████████████████████████████████████████████▉                                                        | 7600/17095 [3:47:13<4:22:43,  1.66s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████▍                                                       | 7700/17095 [3:49:55<4:17:56,  1.65s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████                                                       | 7800/17095 [3:50:06<3:03:32,  1.18s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████▋                                                      | 7900/17095 [3:50:13<2:10:35,  1.17it/s]\n",
      "\u001b[A%|███████████████████████████████████████████████▎                                                     | 8000/17095 [3:50:19<1:32:50,  1.63it/s]\n",
      "\u001b[A%|███████████████████████████████████████████████▊                                                     | 8100/17095 [3:50:24<1:06:40,  2.25it/s]\n",
      "\u001b[A%|█████████████████████████████████████████████████▍                                                     | 8200/17095 [3:50:31<49:24,  3.00it/s]\n",
      "\u001b[A%|██████████████████████████████████████████████████                                                     | 8300/17095 [3:50:47<41:12,  3.56it/s]\n",
      "\u001b[A%|█████████████████████████████████████████████████▋                                                   | 8400/17095 [3:53:33<1:40:27,  1.44it/s]\n",
      "\u001b[A%|██████████████████████████████████████████████████▏                                                  | 8500/17095 [3:56:37<2:28:50,  1.04s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████▊                                                  | 8600/17095 [3:59:19<2:51:48,  1.21s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████▍                                                 | 8700/17095 [4:02:07<3:09:22,  1.35s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████▉                                                 | 8800/17095 [4:04:42<3:15:05,  1.41s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████▌                                                | 8900/17095 [4:07:27<3:22:30,  1.48s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████▏                                               | 9000/17095 [4:10:05<3:24:10,  1.51s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████▊                                               | 9100/17095 [4:12:44<3:24:39,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped 6100 9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A%|██████████████████████████████████████████████████████▎                                              | 9200/17095 [4:15:20<3:22:47,  1.54s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████▉                                              | 9300/17095 [4:18:03<3:23:59,  1.57s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████▌                                             | 9400/17095 [4:20:56<3:27:10,  1.62s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████▏                                            | 9500/17095 [4:24:02<3:33:54,  1.69s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████▋                                            | 9600/17095 [4:26:44<3:28:23,  1.67s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████▎                                           | 9700/17095 [4:29:15<3:19:55,  1.62s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████▉                                           | 9800/17095 [4:31:31<3:07:39,  1.54s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████▍                                          | 9900/17095 [4:34:22<3:10:52,  1.59s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████▍                                         | 10000/17095 [4:37:04<3:09:13,  1.60s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████                                         | 10100/17095 [4:41:12<3:37:25,  1.86s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████▋                                        | 10200/17095 [4:43:52<3:25:16,  1.79s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████▎                                       | 10300/17095 [4:46:50<3:21:53,  1.78s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████▊                                       | 10400/17095 [4:49:55<3:21:28,  1.81s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████▍                                      | 10500/17095 [4:53:09<3:22:43,  1.84s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████                                      | 10600/17095 [4:55:45<3:10:21,  1.76s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████▌                                     | 10700/17095 [4:58:30<3:04:00,  1.73s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████▏                                    | 10800/17095 [5:01:39<3:06:26,  1.78s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████▊                                    | 10900/17095 [5:04:28<3:00:38,  1.75s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████▎                                   | 11000/17095 [5:07:24<2:58:03,  1.75s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████▉                                   | 11100/17095 [5:11:42<3:19:48,  2.00s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████▌                                  | 11200/17095 [5:14:11<3:01:39,  1.85s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████                                  | 11300/17095 [5:16:41<2:48:24,  1.74s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████▋                                 | 11400/17095 [5:19:44<2:47:55,  1.77s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████████▎                                | 11500/17095 [5:22:36<2:43:42,  1.76s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████████▊                                | 11600/17095 [5:25:13<2:35:26,  1.70s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████▍                               | 11700/17095 [5:27:39<2:26:26,  1.63s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████                               | 11800/17095 [5:29:58<2:17:11,  1.55s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████▌                              | 11900/17095 [5:32:44<2:17:21,  1.59s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████▏                             | 12000/17095 [5:37:05<2:40:46,  1.89s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████▊                             | 12100/17095 [5:39:36<2:28:16,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped 9100 12100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████████████▎                            | 12200/17095 [5:42:02<2:17:27,  1.68s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████████████▉                            | 12300/17095 [5:44:41<2:12:17,  1.66s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████████▌                           | 12400/17095 [5:47:27<2:09:39,  1.66s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████                           | 12500/17095 [5:50:21<2:08:45,  1.68s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████▋                          | 12600/17095 [5:58:37<3:19:45,  2.67s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████████▎                         | 12700/17095 [6:01:12<2:50:39,  2.33s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████████▉                         | 12800/17095 [6:03:45<2:29:32,  2.09s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████████████████▍                        | 12900/17095 [6:06:19<2:14:32,  1.92s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████████████                        | 13000/17095 [6:08:57<2:04:17,  1.82s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████████████▋                       | 13100/17095 [6:13:41<2:21:45,  2.13s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████████▏                      | 13200/17095 [6:16:11<2:05:48,  1.94s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████████▊                      | 13300/17095 [6:18:40<1:54:13,  1.81s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████████████▍                     | 13400/17095 [6:21:28<1:48:44,  1.77s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████████████▉                     | 13500/17095 [6:24:15<1:44:11,  1.74s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████████████████████▌                    | 13600/17095 [6:26:57<1:39:13,  1.70s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████████████████▏                   | 13700/17095 [6:29:38<1:34:40,  1.67s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████████████████▋                   | 13800/17095 [6:34:11<1:49:23,  1.99s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████████████▎                  | 13900/17095 [6:36:51<1:39:44,  1.87s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████████████▉                  | 14000/17095 [6:39:43<1:34:20,  1.83s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████████████████▍                 | 14100/17095 [6:42:50<1:31:47,  1.84s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████████████████████████                 | 14200/17095 [6:45:35<1:26:03,  1.78s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████████████████████████▋                | 14300/17095 [6:48:17<1:20:47,  1.73s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████████████████████▏               | 14400/17095 [6:51:25<1:19:53,  1.78s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████████████████████▊               | 14500/17095 [6:54:06<1:14:42,  1.73s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████████████████▍              | 14600/17095 [6:56:40<1:09:28,  1.67s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████████████████▉              | 14700/17095 [6:59:08<1:04:25,  1.61s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████████████████████▌             | 14800/17095 [7:01:48<1:01:37,  1.61s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████████████████████████▉             | 14900/17095 [7:04:34<59:25,  1.62s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████████████████████▍            | 15000/17095 [7:07:14<56:25,  1.62s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████████████████████████            | 15100/17095 [7:09:46<52:49,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped 12100 15100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████████████████████████▋           | 15200/17095 [7:12:17<49:25,  1.56s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████████████████████████████████▎          | 15300/17095 [7:14:55<46:57,  1.57s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████████████████████████████████▉          | 15400/17095 [7:17:30<44:11,  1.56s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████████████████████████████▍         | 15500/17095 [7:20:07<41:34,  1.56s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████████████████████████         | 15600/17095 [7:22:49<39:26,  1.58s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████████████████████████▋        | 15700/17095 [7:25:29<36:52,  1.59s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████████████████████████████▎       | 15800/17095 [7:28:06<34:08,  1.58s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████████████████████████████▊       | 15900/17095 [7:30:36<31:02,  1.56s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████████████████████████████████████▍      | 16000/17095 [7:33:08<28:12,  1.55s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████████████████████████████████      | 16100/17095 [7:36:00<26:30,  1.60s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████████████████████████████████▋     | 16200/17095 [7:38:51<24:20,  1.63s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████████████████████████████▎    | 16300/17095 [7:41:28<21:22,  1.61s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████████████████████████████▊    | 16400/17095 [7:44:15<18:52,  1.63s/it]\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████████████████████████████████▍   | 16500/17095 [7:47:03<16:19,  1.65s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████████████████████████████████████████   | 16600/17095 [7:48:52<12:12,  1.48s/it]\n",
      "\u001b[A%|███████████████████████████████████████████████████████████████████████████████████████████████████▋  | 16700/17095 [7:51:34<10:01,  1.52s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 16800/17095 [7:54:15<07:36,  1.55s/it]\n",
      "\u001b[A%|████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 16900/17095 [7:56:52<05:02,  1.55s/it]\n",
      "\u001b[A%|█████████████████████████████████████████████████████████████████████████████████████████████████████▍| 17000/17095 [7:59:35<02:29,  1.58s/it]\n",
      "17100it [8:02:11,  1.69s/it]                                                                                                                       \n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "offset = 0\n",
    "batch_size = 100\n",
    "num_batches = math.ceil((len(images)-offset)/batch_size)\n",
    "dump_size = 30\n",
    "print(f\"Start transcription of {len(images[offset:])} in {num_batches} batches\")\n",
    "pbar = tqdm(total=len(images)-offset)\n",
    "for batch_idx in range(num_batches):\n",
    "    image_idx = lambda idx_in_batch: batch_size*batch_idx + offset + idx_in_batch\n",
    "    jobs = [\n",
    "        {\n",
    "            \"image\": images[image_idx(idx_in_batch)],\n",
    "            \"content\": build_prompt(images[image_idx(idx_in_batch)])\n",
    "        }\n",
    "        for idx_in_batch in range(batch_size) if image_idx(idx_in_batch) < len(images)\n",
    "    ]\n",
    "    batch_responses = await async_generate_batch(jobs)\n",
    "    responses = responses + batch_responses\n",
    "    pbar.update(batch_size) \n",
    "    # dump every dump_size responses\n",
    "    if batch_idx % dump_size == 0 and batch_idx !=0:\n",
    "        responses_start_idx = batch_size*(batch_idx-dump_size) + offset+batch_size\n",
    "        responses_end_idx = batch_size*batch_idx + offset+batch_size\n",
    "        with open(f\"transcriptions/transcriptions_{responses_start_idx}_{responses_end_idx}.pkl\", \"wb\") as file:\n",
    "            pickle.dump(responses, file)\n",
    "        responses = []\n",
    "        print(f\"Dumped {responses_start_idx} {responses_end_idx}\")\n",
    "pbar.close()\n",
    "# dump remaining responses\n",
    "last_batch_start_idx = int(num_batches/dump_size)*dump_size\n",
    "last_batch_end_idx = num_batches\n",
    "responses_start_idx = batch_size*last_batch_start_idx + offset+batch_size\n",
    "responses_end_idx = batch_size*last_batch_end_idx + offset+batch_size\n",
    "with open(f\"transcriptions/transcriptions_{responses_start_idx}_{responses_end_idx}.pkl\", \"wb\") as file:\n",
    "    pickle.dump(responses, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86a96b26-8786-40bc-a6ab-c0e41c0ba696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load responses\n",
    "responses = []\n",
    "for fname in os.listdir(\"transcriptions/\"):\n",
    "    if not \".pkl\" in fname:\n",
    "        continue\n",
    "    with open(f\"transcriptions/{fname}\",\"rb\") as f:\n",
    "        responses += pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48ae6ed8-25e1-441f-8907-028acd7351c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 generation failures and 0 parsing failures\n"
     ]
    }
   ],
   "source": [
    "# check for parsing errors\n",
    "parsing_failures = []\n",
    "generation_failures = []\n",
    "for idx, job in enumerate(responses):\n",
    "    if \"generate_failure\" in job and job[\"generate_failure\"]:\n",
    "        generation_failures.append((idx, job))\n",
    "    if \"parsing_failure\" in job and job[\"parsing_failure\"]:\n",
    "        parsing_failures.append((idx, job))\n",
    "print(f\"{len(generation_failures)} generation failures and {len(parsing_failures)} parsing failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dcbd4d-8afa-4222-b025-8085fe6c0871",
   "metadata": {},
   "source": [
    "# rerun for not correctly parsed responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3f64a02-a647-484c-a386-9e9558826014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 17095/17095 [28:23<00:00, 10.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# rerun failed\n",
    "did_not_regenerate = []\n",
    "for idx, job in tqdm(enumerate(responses),total=len(responses)):\n",
    "    if (\"generate_failure\" in job and job[\"generate_failure\"]):# or (\"parsing_failure\" in job and job[\"parsing_failure\"]):\n",
    "        try:\n",
    "            job = await async_generate(job, iter(generation_configs))\n",
    "        except:\n",
    "            did_not_regenerate.append(job)\n",
    "        responses[idx] = job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa4c3dff-5f8d-4743-baf6-f9ef8f9c721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 17095/17095 [00:00<00:00, 440692.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# fix generated responses with wrong flag\n",
    "for idx, job in tqdm(enumerate(responses),total=len(responses)):\n",
    "    if (\"generate_failure\" in job and job[\"generate_failure\"]):\n",
    "        try:\n",
    "            response_data = parse_repair_and_validate_response(job[\"response\"])\n",
    "            job[\"data\"] = response_data\n",
    "            if job[\"data\"].shape[0]> 0:\n",
    "                job[\"parsing_failure\"] = False\n",
    "                job[\"generate_failure\"] = False\n",
    "                responses[idx] = job\n",
    "        except:  \n",
    "            responses[idx] = job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e798faa-7d42-4375-ac60-39be681a7042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/17095 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# rerun max token reached\n",
    "MAX_TOKENS_REACHED = 2\n",
    "RECITATION = 4\n",
    "did_not_regenerate = []\n",
    "for idx, job in tqdm(enumerate(responses),total=len(responses)):\n",
    "    finish_reason = job[\"response\"].candidates[0].finish_reason\n",
    "    if \"response\" in job and finish_reason==RECITATION:\n",
    "        try:\n",
    "            job = await async_generate_attempt(job, default_big_generation_config)\n",
    "            job = postprocess_response(job)\n",
    "            responses[idx] = job\n",
    "        except:\n",
    "            did_not_regenerate.append(job)\n",
    "print(f\"failed {len(did_not_regenerate)}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74cefb66-4cfb-4797-aff7-b8059431f298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A%|                                                   | 0/17095 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████| 17095/17095 [00:00<00:00, 139700.08it/s]\n",
      "\n",
      "  0%|                                                    | 0/2084 [01:22<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start regenerating 2084 documents in 21 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A%|██                                        | 100/2084 [00:30<10:00,  3.30it/s]\n",
      "\u001b[A%|████                                      | 200/2084 [01:00<09:32,  3.29it/s]\n",
      "\u001b[A%|██████                                    | 300/2084 [01:29<08:46,  3.39it/s]\n",
      "\u001b[A%|████████                                  | 400/2084 [01:58<08:13,  3.41it/s]\n",
      "\u001b[A%|██████████                                | 500/2084 [02:27<07:46,  3.39it/s]\n",
      "\u001b[A%|████████████                              | 600/2084 [02:58<07:23,  3.35it/s]\n",
      "\u001b[A%|██████████████                            | 700/2084 [03:30<07:02,  3.28it/s]\n",
      "\u001b[A%|████████████████                          | 800/2084 [04:01<06:34,  3.25it/s]\n",
      "\u001b[A%|██████████████████▏                       | 900/2084 [04:32<06:03,  3.26it/s]\n",
      "\u001b[A%|███████████████████▋                     | 1000/2084 [05:02<05:32,  3.26it/s]\n",
      "\u001b[A%|█████████████████████▋                   | 1100/2084 [05:32<05:00,  3.28it/s]\n",
      "\u001b[A%|███████████████████████▌                 | 1200/2084 [06:03<04:30,  3.27it/s]\n",
      "\u001b[A%|█████████████████████████▌               | 1300/2084 [06:34<04:00,  3.26it/s]\n",
      "\u001b[A%|███████████████████████████▌             | 1400/2084 [07:15<03:50,  2.97it/s]\n",
      "\u001b[A%|█████████████████████████████▌           | 1500/2084 [07:46<03:11,  3.05it/s]\n",
      "\u001b[A%|███████████████████████████████▍         | 1600/2084 [08:16<02:35,  3.12it/s]\n",
      "\u001b[A%|█████████████████████████████████▍       | 1700/2084 [08:46<02:00,  3.19it/s]\n",
      "\u001b[A%|███████████████████████████████████▍     | 1800/2084 [09:17<01:28,  3.20it/s]\n",
      "\u001b[A%|█████████████████████████████████████▍   | 1900/2084 [09:46<00:56,  3.25it/s]\n",
      "\u001b[A%|███████████████████████████████████████▎ | 2000/2084 [10:16<00:25,  3.27it/s]\n",
      "\u001b[A0it [10:46,  3.30it/s]                                                         "
     ]
    }
   ],
   "source": [
    "# rerun max token reached batched\n",
    "MAX_TOKENS_REACHED = 2\n",
    "did_not_generate = []\n",
    "for idx, job in tqdm(enumerate(responses),total=len(responses)):\n",
    "    if \"response\" in job and job[\"response\"].candidates[0].finish_reason == MAX_TOKENS_REACHED:\n",
    "        did_not_generate.append((idx, job))\n",
    "BATCH_SIZE = 100\n",
    "num_batches = math.ceil(len(did_not_generate)/BATCH_SIZE)\n",
    "pbar = tqdm(total=len(did_not_generate))\n",
    "print(f\"start regenerating {len(did_not_generate)} documents in {num_batches} batches\")\n",
    "for batch_idx in range(num_batches):\n",
    "    image_idx = lambda idx_in_batch: BATCH_SIZE*batch_idx + idx_in_batch\n",
    "    jobs = [\n",
    "        did_not_generate[image_idx(idx_in_batch)][1]\n",
    "        for idx_in_batch in range(BATCH_SIZE) if image_idx(idx_in_batch) < len(did_not_generate)\n",
    "    ]\n",
    "    response_indices = [\n",
    "        did_not_generate[image_idx(idx_in_batch)][0]\n",
    "        for idx_in_batch in range(BATCH_SIZE) if image_idx(idx_in_batch) < len(did_not_generate)\n",
    "    ]\n",
    "    batch_responses = await async_generate_batch_no_rerun(jobs)\n",
    "    for idx, job in zip(response_indices, batch_responses):\n",
    "        responses[idx] = job\n",
    "    pbar.update(BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0dea4c9-9eea-490e-a7db-9d1ed904b8d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try the parsing with repair\n",
    "i = []\n",
    "for idx, job in enumerate(responses):\n",
    "    if \"generate_failure\" not in job or job[\"generate_failure\"] == False:\n",
    "        try:\n",
    "            response_data = parse_repair_and_validate_response(job[\"response\"])\n",
    "            job[\"data\"] = response_data\n",
    "            job[\"parsing_failure\"] = False\n",
    "            responses[idx] = job\n",
    "        except:\n",
    "            i.append(job)     \n",
    "            responses[idx] = job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6f614-fdc1-44a6-afad-03853e6e124c",
   "metadata": {},
   "source": [
    "# save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed1f7703-da34-4a3a-9e5e-e8053817978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"transcriptions/transcriptions_tmp.pkl\", \"wb\") as file:\n",
    "    pickle.dump(responses,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95cab1a6-d0ce-4427-9619-ccefa6bb692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"transcriptions/transcriptions_tmp.pkl\", \"rb\") as file:\n",
    "    responses = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14133a-bf62-477c-81e8-4d5ef4c86165",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_token_count = 0\n",
    "output_token_count = 0\n",
    "#cost_cutoff = lambda cost: math.ceil(cost*100)/100\n",
    "for response in [response]:\n",
    "    usage_metadata = response.usage_metadata\n",
    "    prompt_token_count += usage_metadata.prompt_token_count\n",
    "    output_token_count += usage_metadata.candidates_token_count\n",
    "print(f\"\"\"\n",
    "Spent {prompt_token_count*0.075*pm + output_token_count*0.4*pm} €\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

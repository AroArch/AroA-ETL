{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10589312-d632-4b91-8714-fc13ed147e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103efb7a-a7e1-40e3-9ac4-9cee1afdce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501aed8-59d3-4406-b5ed-83e0b2dab748",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "518d0275-d48a-4aeb-8f44-4ca47bcff9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce641a274864c6f9391c7170a8367f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8229dcd23020481d95eb592a92363ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/335 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cc95de5ab2476ea8c86ba034b95210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"labels\", data_dir=\"./\")\n",
    "labels = dataset[\"train\"].features[\"label\"].names\n",
    "num_labels=len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103788be-8c31-4578-abe0-640a467003ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i:dataset[\"train\"].features[\"label\"].names[i] for i in range(num_labels)}\n",
    "label2id = {label:idx for idx,label in id2label.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff2fd0d-f93b-4a37-a1de-c0790bfd25a9",
   "metadata": {},
   "source": [
    "## Transform and Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe1da32-4a4e-4d0a-a5dc-d6e923063176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor, Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038be90a-9c63-4ab0-a9fa-fc34a18d8eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ff9fc963174368b848d3324ae84c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c212ca8e3f24f70a446a7ac024bfb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ffddf0e1-314f-4b9c-9585-a8a8ded6c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "\n",
    "_transforms = Compose([\n",
    "    Resize(size),#RandomResizedCrop(size), \n",
    "    ToTensor(), \n",
    "    normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9f0f78f-8af8-4b97-b536-732177a75198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    examples[\"image_names\"] = [img.filename.split(\"/\")[-1] for img in examples[\"image\"] ]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d5eadde-ecd0-422a-9844-d7cbccf129d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "587a4275-142e-4fcd-9030-c915b00388cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee9900-fa25-4cbb-838c-382d1f39eda8",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da0962d4-d9f0-437b-9796-0bbd9900684a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27554af6dbdc4bc89e9406e27d690da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9f8876c-70bd-4095-a655-68eec639d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcb21f9d-3f82-43cb-8ccb-99b3a575f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c427f0e-4161-4214-8d5e-ded505ab1b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78f8ebf9-2128-4b41-aa23-6ef34f0554fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0916c268-1887-4c19-a0df-5db03f851532",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47e670bf-4b7a-43b3-9ad7-8f8e16bba326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35528c0f-03ab-4def-8180-8703eef938ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./image_classifier\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=25,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"train\"],\n",
    "    processing_class=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88b32ebf-0d2b-47f3-94ba-c658c97540a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 07:18, Epoch 20/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.767505</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.668000</td>\n",
       "      <td>1.452908</td>\n",
       "      <td>0.788060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.668000</td>\n",
       "      <td>1.137420</td>\n",
       "      <td>0.785075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.124000</td>\n",
       "      <td>0.907603</td>\n",
       "      <td>0.805970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.779100</td>\n",
       "      <td>0.744653</td>\n",
       "      <td>0.880597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.779100</td>\n",
       "      <td>0.625319</td>\n",
       "      <td>0.895522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.615700</td>\n",
       "      <td>0.534103</td>\n",
       "      <td>0.934328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.615700</td>\n",
       "      <td>0.463122</td>\n",
       "      <td>0.958209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.424300</td>\n",
       "      <td>0.411647</td>\n",
       "      <td>0.958209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.352500</td>\n",
       "      <td>0.369873</td>\n",
       "      <td>0.979104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.352500</td>\n",
       "      <td>0.334086</td>\n",
       "      <td>0.973134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.303521</td>\n",
       "      <td>0.985075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.281212</td>\n",
       "      <td>0.994030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.264280</td>\n",
       "      <td>0.994030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.227600</td>\n",
       "      <td>0.250826</td>\n",
       "      <td>0.994030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.227600</td>\n",
       "      <td>0.240606</td>\n",
       "      <td>0.994030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.994030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.227068</td>\n",
       "      <td>0.997015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.196900</td>\n",
       "      <td>0.223201</td>\n",
       "      <td>0.997015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.220423</td>\n",
       "      <td>0.997015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.5184059734344483, metrics={'train_runtime': 441.1309, 'train_samples_per_second': 18.985, 'train_steps_per_second': 0.283, 'total_flos': 5.4401814531477504e+17, 'train_loss': 0.5184059734344483, 'epoch': 20.952380952380953})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ca0f253-607d-48e7-bcd9-d899d8329231",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bb4ba83-a0bc-4f9f-bd33-1b54f3cf3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"image_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c366a9-1941-4cac-85bc-95172a9f6cac",
   "metadata": {},
   "source": [
    "## validate per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b54b5faf-53d7-476a-be63-092d394756d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec948b-a4b7-4977-a08c-cdd44488841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"labels\"\n",
    "classes_dirs = [d for d in os.listdir(data_path) if \".\" not in d ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24b026-f706-47b8-96b6-8a69f5322a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_at(class_name,img_name):\n",
    "    img = Image.open(f\"{data_path}/{class_name}/{img_name}\")\n",
    "    pixel_values = _transforms(img.convert(\"RGB\")).to(device)\n",
    "    #pixel_values = torch.from_numpy(image_processor(img.convert(\"RGB\"))[\"pixel_values\"][0]).to(device)\n",
    "\n",
    "    outputs = None\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values.unsqueeze(0))\n",
    "    #return outputs\n",
    "    prediction = outputs.logits[0].argmax()\n",
    "    prediction_label = id2label[int(prediction)]\n",
    "    img.close()\n",
    "    return prediction_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8ec3c-b7de-4110-ab80-7a4aebccfd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = dict()\n",
    "for cl in tqdm(classes_dirs):\n",
    "    class_results = []\n",
    "    for img in [img for img in os.listdir(f\"{data_path}/{cl}\") if \".jpg\" in img]:\n",
    "        out = predict_image_at(cl,img)\n",
    "        class_results.append(1 if out == cl else 0)\n",
    "    cl_result = np.array(class_results).mean()\n",
    "    performance[cl] = cl_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc921c40-fc75-4d29-902c-845465179cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67968e1e-3809-4f87-b84e-7da7f08967ba",
   "metadata": {},
   "source": [
    "## classify images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8b4445b-8871-4dba-b20c-58768f9670de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "loaded_model = AutoModelForImageClassification.from_pretrained(\"./model.pt\").to(device)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4e032a3-27ac-4768-82c0-28984ff6f8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bb4efc5e414e80a91acca52dfd1302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/17095 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unlabeled_data = load_dataset(\"images\", data_dir=\"./\",)\n",
    "unlabeled_data = unlabeled_data.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "152f9753-4540-4157-9cbb-2803953d46fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataloader = DataLoader(\n",
    "        unlabeled_data['train'],\n",
    "        batch_size=128,\n",
    "        shuffle=False,  # Don't shuffle for consistent processing\n",
    "        drop_last=False  # Keep all images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5245862-1877-4477-996a-c45b7885ca1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac7569-152a-466d-8170-9bbb59b9e991",
   "metadata": {},
   "source": [
    "## save clustering dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ccf4e4c-94e5-432a-9296-0ff4d05fd5e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [09:40<00:00,  4.33s/it]\n"
     ]
    }
   ],
   "source": [
    "clustering = {i:[] for i in id2label.keys()}\n",
    "with torch.no_grad():\n",
    "    for image_batch in tqdm(unlabeled_dataloader):\n",
    "        pixel_values = image_batch[\"pixel_values\"].to(device)\n",
    "        output = model(pixel_values)\n",
    "        class_ids = output.logits.argmax(dim=1)\n",
    "        for class_id, fname in zip(class_ids, image_batch[\"image_names\"]):\n",
    "            clustering[int(class_id)].append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54f83b25-4c0a-4bfe-a454-306b250a2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "img2cluster = {img: id2label[idx] for idx, cluster in clustering.items() for img in cluster}\n",
    "with open(\"clustering.pkl\",\"wb\")as f:\n",
    "    pickle.dump(img2cluster, f )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45783de1-6619-416d-bf65-b4ea7bc151ba",
   "metadata": {},
   "source": [
    "## save clustered images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03898f4b-aa3a-4673-8076-468991f097dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, label in id2label.items():\n",
    "    os.makedirs(f\"clustering/{label}\",exist_ok=True)\n",
    "img2cluster = {img: idx for idx, cluster in clustering.items() for img in cluster}\n",
    "for img, idx in tqdm(img2cluster.items()):\n",
    "    label = id2label[idx]\n",
    "    Image.open(f\"data/{img_name}\").save(f\"clustering/{label}/{img_name}\")\n",
    "    img_back_name = img_name.replace(\"_1.jpg\",\"_2.jpg\")\n",
    "    if os.path.exists(f\"data/{img_back_name}\"):\n",
    "        Image.open(f\"{path}/{imgname}\").save(f\"clustered/{label}/{img_back_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bbb326-271c-4e01-9f5d-6cfe9f62a0d9",
   "metadata": {},
   "source": [
    "## Inspect Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cacc32-d723-4f71-b526-bb91253c4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(f\"path/to/img.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf236da-a02e-48d4-a483-f47caaa95bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values1 = torch.from_numpy(image_processor(img.convert(\"RGB\"))[\"pixel_values\"][0]).to(device)\n",
    "pixel_values2 = _transforms(img.convert(\"RGB\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f03889-6bde-4296-83e1-1e27c8bce67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_unlabeled_image(img):\n",
    "    pixel_values = _transforms(img.convert(\"RGB\")).to(device)\n",
    "    #pixel_values = torch.from_numpy(image_processor(img.convert(\"RGB\"))[\"pixel_values\"][0]).to(device)\n",
    "\n",
    "    outputs = model(pixel_values.unsqueeze(0))\n",
    "    #return outputs\n",
    "    prediction = outputs.logits[0].argmax()\n",
    "    prediction_label = id2label[int(prediction)]\n",
    "    return prediction_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab4a7b-9439-4256-ba93-676d6ded600b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(loaded_model(pixel_values1.unsqueeze(0)).logits.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b30e5-e616-4cd8-b4b2-d0667701a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(loaded_model(pixel_values2.unsqueeze(0)).logits.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25556d08-1a75-4554-904e-670ebaf59117",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a091d3-568a-4be1-bd88-9d28c85bae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][\"label\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08572a80-4e57-453d-bb06-33563773c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = d[\"image\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
